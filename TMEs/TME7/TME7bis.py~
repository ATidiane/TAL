# -*- coding: utf-8 -*-

import re
import string
import codecs
import numpy as np
from collections import *
import sklearn.naive_bayes as nb
from sklearn import svm
from sklearn import linear_model as lin
import sklearn.feature_extraction.text as txt
from nltk.corpus import stopwords
from nltk import word_tokenize          
from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer
from tools import * # import du fichier d√©fini plus haut

path2train = "corpus.tache1.learn.utf8"
path2test = "corpus.tache1.test.utf8"


class LemmaTokenizer(object):
    def __init__(self):
        self.wnl = WordNetLemmatizer()
        self.snowball_stemmer = SnowballStemmer('french')
        #self.snowball_stemmer.stem(t)
    def __call__(self, doc):
        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]

def readfile(path):
    """
    """
    with open(path, "r") as f:
        punc = string.punctuation
        one = f.read()
        databrut = one.split("\n")
        data = np.array([d.split(' ', 1) for d in databrut])
        datay = np.array([d[0] for d in data])
        datax = np.array([d[-1] for d in data])

    return one, datax[:-1], datay[:-1]

def processing_datay(datay):
    """
    """
    datay = [re.sub('.*<[0-9]*:[0-9]*:C>', '0', dy) for dy in datay]
    datay = [re.sub('.*<[0-9]*:[0-9]*:M>', '1', dy) for dy in datay]
    # -1 cause the last one is equal to ''
    return np.array(datay, int)

def countCM(path):
    with open(path, 'r') as f:
        d = np.array(f.read().split("\n"))
        chirac = np.where(d == 'C')[0]
        mitterand = np.where(d == 'M')[0]

    return chirac.shape, mitterand.shape
