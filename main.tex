\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}

%Packages Mise en page
%I dont like the default margin so i modified it here
\usepackage[margin=2cm]{geometry}
%\usepackage{fancyhdr} %gestion des ET et PDP
%\usepackage{enumitem} %gestion des puces de listes.
%\usepackage{titlesec} %gestion des espaces avant et aprÃ¨s les sections et sous sections
\usepackage{ragged2e} % Pour mise en page du texte, comme justify par exemple
\usepackage{etoolbox} % I dont know what it is for, look after

%Packages Tableaux
\usepackage{tabularx} %Tableaux
\usepackage{multirow} %Gestion des lignes
\usepackage{multicol} %Gestion des colones
\usepackage{arydshln} %Lignes en pointillÃ©s
\usepackage{fancybox} %Boites
\usepackage{multicol} %Colonnes
\usepackage{array} %Tableaux maths
\usepackage{fancybox}
\usepackage{diagbox}
\usepackage{mathabx}

%Packages Figures et graphiques
\usepackage{graphics} %inclusion de figures
\usepackage{graphicx} %inclusion de figures
\usepackage{pstricks-add}

%Packages représentations des lois normales et binomiales
\usepackage{pst-func}

\usepackage{hyperref} %pour insérer des liens 


\apptocmd{\document}{}{\justifying}{}

\title{Note of Multiword Unit Hybrid Extraction }
\author{ahmedtidianebalde }
\date{February 2018}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\setlength{\parindent}{0cm} %Pour enlever l'indentation au debut des paragraphes, que Ã§a me saoulait, punaise! Ouffffff, enfin.

\section{Introduction}

    \subsection{Informations de l'article}
    
        \textbf{Titre} : MultiWord Unit Hybrid Extraction 
        
        \textbf{Auteur} : Gaël Dias
        
        \textbf{Conférence} : In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment-Volume 18 (pp. 41-48). Association for Computational Linguistics.
        
        \textbf{Date} : 2003, July
    
    \subsection{Définition des termes principaux}
        
        \textbf{Multiword Unit Extraction} : Extraction d'unités polylexicales.
        
        \textbf{Unités polylexicales} : suites de mots qui se trouvent plus fréquemment associés qu'ils ne le seraient par le 
        seul fruit du hasard.
        
        \textbf{Unités morphologiques} : suites de caractères qui se trouvent plus fréquemment associés qu'ils ne le seraient par le seul fruit du hasard.
        
        \textbf{Locution} : groupe de mots dont la syntaxe particulière donne à ces groupes le caractère d'une expression figée et qui correspondent à des mots uniques. On distingue les locutions verbales \lbrack faire grâce \rbrack, nominales \lbrack mise en jeu \rbrack, adverbiales \lbrack tout de suite \rbrack, prépositives\lbrack au-dessus de \rbrack, conjonctives \lbrack pour que \rbrack.)
        
        \textbf{Lemmatisation} : consiste tout simplement à prendre la forme canonique de l'ensemble de la forme d'un mot.
        
        \textbf{phrasal verbs} : verbe à particule qui est un mot composé constitué de verbe de base associé à un élément d'origine non verbale (la \textit{particule}) qui en complète ou modifie le sens.
        
        \textbf{ngram} : n mots.( vecteurs ordonn\'es d'unit\'es de textes)
        
        \textbf{corpora} : corpus au pluriel
        
        \textbf{Expectative Mutuelle} : un nouveau mod\`ele probabiliste , qui mesure le degr\'e d'association qui lie entre eux tous les \'el\'ements d'un n-gram (i.e. $\forall n, n \geq 2 $) et permet ainsi d'acqu\'erir des associations n-aires sans recourir aux techniques d'amor\c cage. 
        
\section{Analyse}

    \subsection{Objectif de l'article : Problématique abordée et positionnement}
        Cet article propose une système hybride , nomm\'e HELAS(Extraction Hybride d'associations lexicaux) ,capable d'extraire automatiquement des associations textuelles en l'occurrence, les unités polylexicales, incluant les \textbf{noms composés} (e.g. \textit{interior designer}), \textbf{les locutions adverbiales et prépositives} . En effet, ceci revêt d'une importance cruciale pour le succès de nombreuses applications dans le domaine du traitement du langage naturel.
    
    \subsection{Difficulté de la tâche}
    
 Il existe 3 diff\'erentes m\'ethodologies pour extraire les MWU: Syntaxique, Statistique,Hybride: Syntaxico-statistique. Cependant ces m\'ethodes pr\'esentent de diff\'erentes difficult\'es: \\
 \begin{enumerate}
 \item Syntaxique: \\
 Il exige fortement des techniques linguistiques sp\'ecialis\'es pour pouvoir identifier les indices qui isolent les MWU.
 \item Statistique:\\
Ils peuvent seulement identifier des associations textuelles dans le contexte de leur utilisation. En cons\'equence, beaucoup de structures pertinentes ne peuvent pas \^etre pr\'esent\'ees directement dans des bases de donn\'ees lexicales comme ils ne garantissent pas de structures linguistiques ad\'equates pour ce but.
 \item Hybride: Syntaxico-statistique:\\
 L'inconv\'enient majeur de ce syst\`eme, est qu'il ne traite pas une grande proportion de MWU int\'eressants(par exemple: locutions pr\'epositives). De plus, il manque en flexibilit\'e, car les structures syntaxiques doivent \^etre revu lorsqu'on change la langue du texte \`a trait\'e.
 \end{enumerate}

    \subsection{Apports du travail par rapport à l'existant}
    
    Cette m\'ethode se diff\'erencie de la majorit\'e des travaux propos\'es par l'absence de pr\'e-traitement du corpus.Ainsi le texte n'est ni lemmatis\'e ni \'etiquet\'e ni \'epur\'e au moyen de "stop-listes". De plus,en \'evitant l'intervention humaine dans la d\'einition de mod\`eles syntaxiques, ce syst\`eme fournit la flexibilit\'e totale. Il ne recourt pas aux m\'ethodes d'amor\c cage ni \`a la d\'efinition de valeurs seuil globales.
    
    \subsection{Méthode / algorithme}
    
    D'une point de vue technique , ce syt\`eme se repose sur une m\'ethode qui peut \^etre d\'ecompos\'ee 4 grandes \'etapes:
\begin{enumerate}
\item \underline{Pr\'eparation des donn\'ees }\\ \\
Le texte/corpus en entr\'ee est divis\'e en 2 sous-corpus:
 \begin{itemize}
 \item Corpus contenant les mots
 \item Corpus contenant les \'etiquettes morpho-syntaxiques/unit\'es morphologiques. \\
 \end{itemize}

\item \underline{ Segmentation du texte} \\ \\
Chaque sous-corpus est segment\'e en un ensemble de ngrams. Puis chaque ngram de mot est associ\'e \`a un ngram d'unit\'e morphologique afin d'\'evaluer le degr\'e d'association du s\'equence de mots. Afin d'\'evaleur le degr\'e de coh\'esivit\'e d'une  s\'equence d'unit\'es de textes, on utilise le mesure d'association nomm\'e "Expectative Mutuelle".

\item \underline{ Evaluation de la coh\'esivit\'e}

 \begin{itemize}
 \item Calcul de l'expectative normalis\'ee(EN) \\ \\
 L'expectative normalis\'ee existant entre n unit\'es textuelles (UT) est d\'efini comme \'etant l'expectative moyenne de voir appar\^itre une UT dans une position donn\'ee sachant que les autres (n-1) UTs apparaissent dans le texte contraintes par leurs positions. L'id\'ee de base est d'\'evaluer le co\^ut de la perte d'une UT dans un n-gram. \\
Soit  $ [p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n} ]$ un ngram, dans lequel p11 vaut z\'ero, u1 identifie l?unit\'e pivot et p1i repr\'esente la distance sign\'ee entre l?unit\'e ui et l?unit\'e pivot.
 La formule de l'EN est donn\'e par l'\'equation 1 ci-dessous.\\
\\ \textbf{Equation 1:}\\
$ EN ([p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n} ])  = \frac{k[((p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n} )]}{\frac{1}{n} \times ( k[((p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n} )] + \sum_{i=2}^{n} k[((p_{11}  u_{1} ... p_{1i}^* u_{i}^* ... p_{1n}  u_{n} )]  )}$
\\ \\ L'un des crit\`eres les plus importants pour l'identification d'associations textuelles est la fr\'equence. Or, l'expectative normalis\'ee mesure le degr\'e de coh\'esion qui lie les constituants d'un n-gram mais ne rend pas compte de l'hypoth\`ese formul\'ee pr\'ec\'edemment. D'o\`u on introduit l'expectative mutuelle.

 \item Calcul de l'expectative mutuelle(EM) (pour les deux sous corpus).
Entre deux n-grams ayant la m\^eme expectative normalis/'ee, il est plus probable que le plus fr\'equent des deux corresponde \'a une association textuelle pertinente. L'expectative mutuelle d'un ngram est d\'efini gr\^ace \^a son expectative normalis\'ee et sa fr\'equence(Equation 2).\\
\\ \textbf{Equation 2:}\\
 $ EM([p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n}])= p([p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n}]) \times EN(p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n}]) $ \\ 
 o\`u p(.) est une fonction donnant la fr\'equence relative. \\
\\  L'expectative mutuelle permet donc de mesurer le degr\'e de coh\'esion de tout n-gram sans \^etre limit\'ee aux associations binaires. Ainsi, il est possible de classer tout n-gram (i.e. $\forall n, n \geqslant 2) $suivant son degr\'e de pertinence.
 
 \item Calcul de la mesure d'association combin\'e (MAC)\ \\
 Pr\'e-d\'efinir les mod\`eles syntaxiques influencent l'extraction des MWU. La solution donn\'e ici pour r\'esoudre ce probl\`eme est de se bas\'e sur l'assumption suivante: Plus les mots   d'un s\'equence et leurs \'etiquettes morphologiques correspondants sont coh\'esifs, plus il est probable qu'il repr\'esente un MWU. Le degr\'e global de coh\'esion d'une s\'equence de mots peut donc \^etre \'evalu\'e comme le produit de son EM avec l'EM de l'\'etiquette associ\'e(Equation 3).  \\
 \\ \textbf{Equation 3:}\\
 $ MAC[p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n}])= EM([p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n}])^\alpha \times EM(p_{11}  u_{1} ... p_{1i}  u_{i} ... p_{1n}  u_{n}])^{\alpha-1 }$ \\ 
  o\`u $\alpha$ est un param\`etre qui accentue la mesure soit sur les mots ou les \'etiquettes.
 \end{itemize}
 
\item \underline{Processus d'acquisition }\\ \\
Le nouvel algorithme,  GenLocalMaxs, ne d\'epend d'aucun seuil (pr\'e-\'etabli ou mesur\'e par exp\'erimentation) et qui \'elit tout vecteur dont le degr\'e d'association correspondant est un maximum local. \\ Soient, une mesure d'association asso, un n-gram W, l'ensemble de tous les (n-1)-grams contenus dans W, $ \Omega_{n-1}$, l'ensemble de tous les (n+1)-grams contenant W, $\Omega_{n+1} $et une fonction taille qui rend la longueur d'un n-gram W donn\'e en argument, alors: \\
\center { $ \forall x \in \Omega_{n-1} ,\forall y \in \Omega_{n+1} $, W est MWU si } \\
$(taille(W) = 2 \land mac(W) > mac(y) ) \lor (taille(W) \neq 2 \land mac(W) \geq mac(x) \land mac(W) > mac(y)) $ 
\end{enumerate}
 
    \subsection{Exp\'erimentation / R\'esultats}
    
    Afin de tester l'architecture, le syst\`{e}me HELAS a \'{e}t\'{e} appliqu\'{e} sur le corpus Brown.\\
Due au manque d'espace de l'ordinateur,le test se limite \`a une partie du corpus contenant 249 578 mots et pour des unit\'es de mots contigues. \\
Les tests se basent sur 11 diff\'erentes valeurs pour $\alpha $, pour $ \alpha \in \{ 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\} $, permettant de voir les diff\'erents MWU obtenus selon si la pertinence d'une s\'equence de mots est bas\'e sur la d\'{e}pendance de ces mots $ ( \alpha = $ 1  )ou sur les \'etiquettes morpho-syntaxiques $ ( \alpha = $ 0  ).

    
    \subsubsection{Analyse Quantitative des r\'esultats}
    
	D'apr\'es les r\'esultats, nous pouvons tir\'e 3 grandes remarques:
	\begin{enumerate}
	\item Le nombre de MWU  et le degr\'e des n-grams atteint/obtenu  au gr\'e des valeurs de $\alpha$. \\
		\\Plus $\alpha$ tend vers 0, plus le nombre de MWU extrait est important. Ceci s'explique sur le fait que lorsque $\alpha$ = 0, tout s\'equence de mot, associ\'e \`a un \'etiquette morpho-syntaxique, est extrait ind\'ependamment des mots qu'il contient.L'inclusion du facteur de mot,en augmentant la valeur de $\alpha$ m\'ene progressivement \`a un nombre diminuant de ngrams positionnel. En effet, les s\'equences de mots sont filtr\'es selon les statistiques de leur mots. \\
		Les \'etiquettes morpho-syntaxiques de longueur 2 sont plus fr\'equentes que les autres. Or les petites valeurs de $\alpha$ traitent plus les structures syntaxiques. D'o\`u , plus la valeur de $\alpha$ est petite, plus il y as de 2grams extrait. 
		
	\item Le nombre de/La proportion des s\'equences extrait communs ( selon les valeurs de $\alpha$).\\ \\
On "ratio identique", c'est \`a dire le quotient entre le nombre  de s\'equences identiques extrait et le nombre de s\'equences diff\'erent extrait. D'apr\`es les r\'esultats, on observe par exemple pour $\alpha$ = 0 et $\alpha$ = 1, il y a plus de s\'equences diif\'erents que de s\'equences identiques. Ce ph\'enom\`ene augmente progressivement lorsqu'on augemente la valeur de $\alpha$.
	
	\item La fr\'equence des s\'equences extrait. 
	\\ \\Il est notable que la plupart des s\'equences extrait ont une fr\'equence de 2. Ce r\'esultat est satisfaisant car la majorit\'e des extracteurs ont besoin de plus haute fr\'equence pour d\'ecider si la s\'equences est un MWU ou non.
	\end{enumerate}
     
     \subsubsection{Analyse Qualitative des r\'esultats}
   Pour classer une s\'equence de mots en MWU, il faut dabour d\'efinir un MWU. La d\'efinition le plus adapt\'e est celui e Gaston Gross. Un MWU est l'un parmi les suivants: 
   nom compos\'e ,d\'eterminant compos\'e, la locution verbale , la locution adverbiale, la locution adjectivale  ou la locution pr\'epositionnelle.\\
   Les r\'esultats de pr\'ecision montrent que les d/'ependances de mots et des d\'ependances d'\'etiquettes peuvent les deux, jouer un r\^ole important dans l'identification des s\'equences pertinents. \\Pour $\alpha$ = 0.5, on obtient une pr\'ecision de 62\% , ce qui signifie donc les deux contribuent \'egalement.
 Le syst\`eme ne semble pas pouvoir aborder avec succ\`es des MWU avec plus de trois mots.Cependant , les d\'equences de 2 mots restent un probl\`eme pour cet analyseur/extracteur.
 
    \subsection{Intérêt de l'article}

Cet article, nous pr\'esente le syst\`eme hybride qui extrait des MWU en identifiant les mod\`eles syntaxiques pertinents d'un corpus et en combinant les statistiques des mots avec les informations linguistiques acquises. La m\'ethode de ce syst\`eme est d\'ecrit en grandes \'etapes. De plus, \`a travers diff\'erents exp\'erimentations et leurs r\'esultats, il illustre la performance et les avantages de ce dernier.

\end{document}
